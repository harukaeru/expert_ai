import streamlit as st
from langchain_openai import ChatOpenAI
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain
import asyncio
import json

class ExpertPanelChatbot:
    def __init__(self, openai_api_key: str, experts: dict, model_config: dict):
        self.llm = ChatOpenAI(
            model_name=model_config.get("model_name", "gpt-4o-mini"),
            temperature=model_config.get("temperature", 0.7),
            openai_api_key=openai_api_key
        )
        
        self.experts = {k: v["description"] for k, v in experts.items()}
        self.expert_chains = self._initialize_expert_chains()
        self.summary_chain = self._initialize_summary_chain()

    def _initialize_expert_chains(self) -> dict:
        expert_chains = {}
        expert_prompt_template = """
        ã‚ãªãŸã¯{expert_role}ã¨ã—ã¦ã€ä»¥ä¸‹ã®è³ªå•/ãƒˆãƒ”ãƒƒã‚¯ã«ã¤ã„ã¦æ„è¦‹ã‚’æä¾›ã—ã¦ãã ã•ã„ã€‚
        ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«å¯¾ã—ã¦ã‚ãªãŸã®ç«‹å ´ãƒ»ã‚ãªãŸã®è¦³ç‚¹ã‹ã‚‰å¾¹åº•çš„ã«è€ƒå¯Ÿã—ã¦ãã ã•ã„ã€‚
        ã‚ã‹ã‚‰ãªã„éƒ¨åˆ†ã«é–¢ã—ã¦ã¯ã€ç´ ç›´ã«ã€Œã‚ã‹ã‚‰ãªã„ã€ã¨ç­”ãˆã¦ãã ã•ã„ã€‚
        
        è³ªå•/ãƒˆãƒ”ãƒƒã‚¯: {question}
        """
        
        for expert_id, expert_desc in self.experts.items():
            prompt = PromptTemplate(
                template=expert_prompt_template,
                input_variables=["expert_role", "question"]
            )
            expert_chains[expert_id] = LLMChain(llm=self.llm, prompt=prompt)
        return expert_chains
    
    def _initialize_summary_chain(self) -> LLMChain:
        summary_prompt_template = """
        ä»¥ä¸‹ã¯ç•°ãªã‚‹å°‚é–€å®¶ã‹ã‚‰æä¾›ã•ã‚ŒãŸæ„è¦‹ã§ã™ã€‚ã“ã‚Œã‚‰ã®æ„è¦‹ã‚’çµ±åˆã—ã€ãƒãƒ©ãƒ³ã‚¹ã®å–ã‚ŒãŸåŒ…æ‹¬çš„ãªçµè«–ã‚’å°ãå‡ºã—ã¦ãã ã•ã„ã€‚
        å…ƒã®è³ªå•/ãƒˆãƒ”ãƒƒã‚¯: {original_question}
        
        å°‚é–€å®¶ã®æ„è¦‹:
        {expert_opinions}
        
        çµ±åˆã•ã‚ŒãŸçµè«–:
        1. ä¸»ãªåˆæ„ç‚¹
        2. é‡è¦ãªæ‡¸å¿µäº‹é …
        3. æ¨å¥¨ã•ã‚Œã‚‹ã‚¢ã‚¯ã‚·ãƒ§ãƒ³
        4. è¿½åŠ ã§æ¤œè¨ãŒå¿…è¦ãªç‚¹
        
        çµè«–:
        """
        
        summary_prompt = PromptTemplate(
            template=summary_prompt_template,
            input_variables=["original_question", "expert_opinions"]
        )
        return LLMChain(llm=self.llm, prompt=summary_prompt)

    async def get_expert_opinion(self, expert_id: str, expert_desc: str, question: str, progress_placeholder) -> str:
        """å€‹ã€…ã®å°‚é–€å®¶ã®æ„è¦‹ã‚’å–å¾—ã—ã€ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã§è¡¨ç¤ºã™ã‚‹"""
        progress_placeholder.markdown(f"ğŸ¤” **{expert_id.replace('_', ' ').title()}** ãŒè€ƒãˆã¦ã„ã¾ã™...")
        
        response = await self.expert_chains[expert_id].arun(
            expert_role=expert_desc,
            question=question
        )
        
        # å°‚é–€å®¶ã®å›ç­”ã‚’æ•´å½¢ã—ã¦è¡¨ç¤º
        formatted_response = f"""
        ### ğŸ’¡ {expert_id.replace('_', ' ').title()}ã®æ„è¦‹:
        
        {response}
        
        ---
        """

        progress_placeholder.markdown(formatted_response)
        return response
    
    async def get_integrated_response(self, question: str, expert_opinions: dict, progress_placeholder) -> tuple:
        """å°‚é–€å®¶ã®æ„è¦‹ã‚’åé›†ã—ã€çµ±åˆã•ã‚ŒãŸå›ç­”ã‚’ç”Ÿæˆã™ã‚‹"""
        # æ„è¦‹ã‚’çµ±åˆ
        formatted_opinions = "\n\n".join([
             f"{expert_id.upper()}ã®æ„è¦‹:\n{opinion}"
             for expert_id, opinion in expert_opinions.items()
        ])
    
        progress_placeholder.markdown("### ğŸ”„ æœ€çµ‚çš„ãªçµè«–ã‚’ç”Ÿæˆä¸­...")
        final_response = await self.summary_chain.arun(
            original_question=question,
            expert_opinions=formatted_opinions
        )
    
        return final_response

def init_session_state():
    if 'messages' not in st.session_state:
        st.session_state.messages = []
    if 'chatbot' not in st.session_state:
        st.session_state.chatbot = None
    if 'experts' not in st.session_state:
        st.session_state.experts = {
            "graph_specialist": {
                "description": "ã‚°ãƒ©ãƒ•æ§‹é€ ã®å°‚é–€å®¶ã€‚ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚„ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã«è©³ã—ãã„ã¤ã‚‚ãã®è¦³ç‚¹ã§ã‚‚ã®ã”ã¨ã‚’è€ƒãˆã¦åˆ†æã‚’è¡Œã†ã€‚",
                "avatar": "ğŸ•¸ï¸",
                "name": "ã‚°ãƒ©ãƒ•ç†è«–å°‚é–€å®¶"
            },
            "tech_expert": {
                "description": "æŠ€è¡“å°‚é–€å®¶ã€‚æŠ€è¡“çš„ãªå®Ÿç¾å¯èƒ½æ€§ã€å¿…è¦ãªãƒªã‚½ãƒ¼ã‚¹ã€é–‹ç™ºæœŸé–“ã«ã¤ã„ã¦åˆ†æã‚’è¡Œã†ã€‚",
                "avatar": "ğŸ‘¨â€ğŸ’»",
                "name": "æŠ€è¡“ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒˆ"
            },
            "math_expert": {
                "description": "æ•°å­¦è€…ã€‚å¹…åºƒã„åˆ†é‡ã«ãŠã„ã¦ç¿’ç†Ÿã—ã¦ãŠã‚Šã€ã¨ã‚Šã‚ã‘å¿œç”¨æ•°å­¦ã«å¼·ã¿ãŒã‚ã‚‹ã€‚å®šå¼åŒ–ãŒå¥½ãã€‚æ•°å­¦ã®è¦³ç‚¹ã‹ã‚‰ã‚‚ã®ã”ã¨ã‚’åˆ†æã™ã‚‹ã€‚",
                "avatar": "ğŸ“",
                "name": "å¿œç”¨æ•°å­¦è€…"
            },
            "money_hunter": {
                "description": "å¯Œè±ªã‹ã¤çµŒæ¸ˆå­¦è€…ã€‚å¸¸ã«ãã‚ŒãŒè³‡ç”£ã«ãªã‚‹ã‹ã€ã‚ã‚‹ã„ã¯ãã‚ŒãŒè³‡ç”£ã«ãªã‚‹ãŸã‚ã«ä½•ã‚’ã™ã¹ãã‹ã‚’è€ƒãˆã¦åˆ†æã™ã‚‹ã€‚é‡‘èã‚„çµŒæ¸ˆãŒå¤§å¥½ãã€‚",
                "avatar": "ğŸ’°",
                "name": "æŠ•è³‡å®¶å…¼çµŒæ¸ˆå­¦è€…"
            },
            "layman_takehashi": {
                "description": "ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¨ä»²ã®ã„ã„å‹é”ã§ã‚ã‚‹ç«¹æ©‹ã€‚é ­ã¯ã„ã„ãŒã»ã¨ã‚“ã©ã®å ´é¢ã«ãŠã„ã¦ç´ äººã§ã€ç¤¾ä¼šã®ã“ã¨ãŒã‚ˆãã‚ã‹ã£ã¦ã„ãªã„ãŒã€ãã‚Œã§ã‚‚ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«å¯¾ã—ã¦ä¸€ç”Ÿæ‡¸å‘½è‡ªåˆ†ã®ç«‹å ´ã‹ã‚‰å¾¹åº•çš„ã«è€ƒå¯Ÿã—ã¦ãã‚Œã‚‹ã€‚",
                "avatar": "ğŸ™†",
                "name": "ãƒã‚¤ãƒ•ãƒ¬ãƒ³ãƒ‰ç«¹æ©‹"
            },

        }
    if 'model_config' not in st.session_state:
        st.session_state.model_config = {
            "model_name": "gpt-4o-mini",
            "temperature": 0.7
        }

def model_config_manager():
    """ãƒ¢ãƒ‡ãƒ«è¨­å®šã‚’ç®¡ç†ã™ã‚‹UI"""
    st.subheader("ãƒ¢ãƒ‡ãƒ«è¨­å®š")
    
    # åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ã®ãƒªã‚¹ãƒˆ
    available_models = [
        "gpt-4o-mini",  # GPT-4o-mini
        "gpt-4o",               # GPT-4o
    ]
    
    # ãƒ¢ãƒ‡ãƒ«é¸æŠ
    selected_model = st.selectbox(
        "ãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠ",
        available_models,
        index=available_models.index(st.session_state.model_config["model_name"])
    )
    
    # Temperatureè¨­å®š
    temperature = st.slider(
        "Temperature (å‰µé€ æ€§ã®åº¦åˆã„)",
        min_value=0.0,
        max_value=2.0,
        value=st.session_state.model_config["temperature"],
        step=0.1,
        help="ä½ã„å€¤: ã‚ˆã‚Šä¸€è²«æ€§ã®ã‚ã‚‹å¿œç­”\né«˜ã„å€¤: ã‚ˆã‚Šã‚¯ãƒªã‚¨ã‚¤ãƒ†ã‚£ãƒ–ãªå¿œç­”"
    )
    
    # è¨­å®šã®ä¿å­˜
    if selected_model != st.session_state.model_config["model_name"] or \
       temperature != st.session_state.model_config["temperature"]:
        st.session_state.model_config["model_name"] = selected_model
        st.session_state.model_config["temperature"] = temperature
        st.success("ãƒ¢ãƒ‡ãƒ«è¨­å®šãŒæ›´æ–°ã•ã‚Œã¾ã—ãŸï¼")
    
    # ãƒ¢ãƒ‡ãƒ«æƒ…å ±ã®è¡¨ç¤º
    with st.expander("ãƒ¢ãƒ‡ãƒ«æƒ…å ±"):
        st.markdown("""
        **ãƒ¢ãƒ‡ãƒ«ã®ç‰¹å¾´:**
        - **GPT-4o-mini**: é«˜æ€§èƒ½ãªåŸºæœ¬ãƒ¢ãƒ‡ãƒ«ã€‚è¤‡é›‘ãªã‚¿ã‚¹ã‚¯ã«é©ã—ã¦ã„ã¾ã™ã€‚ã‚³ã‚¹ãƒˆã¯å°ç¨‹åº¦ã§ã™ã€‚
        - **GPT-4o**: æœ€æ–°ã§æœ€ã‚‚é«˜æ€§èƒ½ãªãƒ¢ãƒ‡ãƒ«ã€‚ã‚³ã‚¹ãƒˆã¯é«˜ã‚ã§ã™ãŒã€æœ€æ–°ã®çŸ¥è­˜ã¨é«˜ã„å‡¦ç†èƒ½åŠ›ã‚’æŒã¡ã¾ã™ã€‚
        
        **Temperatureè¨­å®šã®å½±éŸ¿:**
        - **0.0-0.3**: ã‚ˆã‚Šäº‹å®Ÿã«åŸºã¥ã„ãŸã€æ±ºå®šè«–çš„ãªå¿œç­”
        - **0.4-0.7**: ãƒãƒ©ãƒ³ã‚¹ã®å–ã‚ŒãŸå¿œç­”
        - **0.8-2.0**: ã‚ˆã‚Šã‚¯ãƒªã‚¨ã‚¤ãƒ†ã‚£ãƒ–ã§å¤šæ§˜ãªå¿œç­”
        """)
    
    # è¨­å®šã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ/ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
    with st.expander("ãƒ¢ãƒ‡ãƒ«è¨­å®šã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ/ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ"):
        # ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
        export_data = json.dumps(st.session_state.model_config, indent=2, ensure_ascii=False)
        st.download_button(
            label="ãƒ¢ãƒ‡ãƒ«è¨­å®šã‚’ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ",
            data=export_data,
            file_name="model_settings.json",
            mime="application/json"
        )
        
        # ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
        uploaded_file = st.file_uploader("ãƒ¢ãƒ‡ãƒ«è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ", type="json")
        if uploaded_file is not None:
            try:
                imported_config = json.load(uploaded_file)
                if "model_name" in imported_config and "temperature" in imported_config:
                    st.session_state.model_config = imported_config
                    st.success("ãƒ¢ãƒ‡ãƒ«è¨­å®šãŒã‚¤ãƒ³ãƒãƒ¼ãƒˆã•ã‚Œã¾ã—ãŸï¼")
                else:
                    st.error("ç„¡åŠ¹ãªè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã§ã™")
            except Exception as e:
                st.error(f"ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼: {str(e)}")


def expert_manager():
    """å°‚é–€å®¶ã®è¨­å®šã‚’ç®¡ç†ã™ã‚‹UI"""
    st.subheader("å°‚é–€å®¶ã®è¨­å®š")
  
    # å°‚é–€å®¶ã®è¿½åŠ 
    with st.expander("å°‚é–€å®¶ã‚’è¿½åŠ "):
        col1, col2, col3 = st.columns(3)
        with col1:
            new_expert_id = st.text_input("å°‚é–€å®¶IDï¼ˆè‹±æ•°å­—ï¼‰", key="new_expert_id")
        with col2:
            new_expert_name = st.text_input("è¡¨ç¤ºå", key="new_expert_name")
        with col3:
            new_expert_avatar = st.text_input("ã‚¢ãƒã‚¿ãƒ¼çµµæ–‡å­—", key="new_expert_avatar")
        new_expert_desc = st.text_area("å°‚é–€å®¶ã®èª¬æ˜", key="new_expert_desc")
        
        if st.button("è¿½åŠ ", key="add_expert"):
            if all([new_expert_id, new_expert_name, new_expert_avatar, new_expert_desc]):
                st.session_state.experts[new_expert_id] = {
                    "description": new_expert_desc,
                    "avatar": new_expert_avatar,
                    "name": new_expert_name
                }
                st.success(f"å°‚é–€å®¶ã€Œ{new_expert_name}ã€ãŒè¿½åŠ ã•ã‚Œã¾ã—ãŸï¼")
  
    # æ—¢å­˜ã®å°‚é–€å®¶ã®ç®¡ç†
    with st.expander("å°‚é–€å®¶ã‚’ç®¡ç†"):
        for expert_id, expert_info in list(st.session_state.experts.items()):
            st.markdown(f"**{expert_info['name']} {expert_info['avatar']}**")
            cols = st.columns(3)
            with cols[0]:
                new_name = st.text_input("è¡¨ç¤ºå", value=expert_info['name'], key=f"name_{expert_id}")
            with cols[1]:
                new_avatar = st.text_input("ã‚¢ãƒã‚¿ãƒ¼", value=expert_info['avatar'], key=f"avatar_{expert_id}")
            new_desc = st.text_area("èª¬æ˜", value=expert_info['description'], key=f"desc_{expert_id}")
            
            col1, col2 = st.columns([1, 1])
            with col1:
                if st.button("æ›´æ–°", key=f"update_{expert_id}"):
                    st.session_state.experts[expert_id] = {
                        "description": new_desc,
                        "avatar": new_avatar,
                        "name": new_name
                    }
                    st.success("æ›´æ–°ã•ã‚Œã¾ã—ãŸï¼")
            with col2:
                if st.button("å‰Šé™¤", key=f"delete_{expert_id}"):
                    del st.session_state.experts[expert_id]
                    st.warning("å‰Šé™¤ã•ã‚Œã¾ã—ãŸï¼")
            st.markdown("---")

    # è¨­å®šã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ/ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
    with st.expander("è¨­å®šã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ/ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ"):
        # ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
        export_data = json.dumps(st.session_state.experts, indent=2, ensure_ascii=False)
        st.download_button(
            label="è¨­å®šã‚’ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ",
            data=export_data,
            file_name="expert_settings.json",
            mime="application/json"
        )
        
        # ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
        uploaded_file = st.file_uploader("è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ", type="json")
        if uploaded_file is not None:
            try:
                imported_experts = json.load(uploaded_file)
                st.session_state.experts = imported_experts
                st.success("è¨­å®šãŒã‚¤ãƒ³ãƒãƒ¼ãƒˆã•ã‚Œã¾ã—ãŸï¼")
            except Exception as e:
                st.error(f"ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼: {str(e)}")

async def main():
    st.title("å°‚é–€å®¶ãƒ‘ãƒãƒ«ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆ ğŸ¤–")
    
    init_session_state()
    
    # ã‚µã‚¤ãƒ‰ãƒãƒ¼ã§è¨­å®š
    with st.sidebar:
        st.header("è¨­å®š")
        
        # OpenAI APIã‚­ãƒ¼è¨­å®š
        api_key = st.text_input("OpenAI APIã‚­ãƒ¼", type="password")
        if api_key:
            st.success("APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¾ã—ãŸï¼")
        
        # ã‚¿ãƒ–ã§è¨­å®šã‚’æ•´ç†
        tab1, tab2 = st.tabs(["ãƒ¢ãƒ‡ãƒ«è¨­å®š", "å°‚é–€å®¶è¨­å®š"])
        
        with tab1:
            model_config_manager()
        
        with tab2:
            expert_manager()  # å‰å›ã®ã‚³ãƒ¼ãƒ‰ã‹ã‚‰å¤‰æ›´ãªã—
    
    # ãƒ¡ã‚¤ãƒ³ã®ãƒãƒ£ãƒƒãƒˆã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹
    for message in st.session_state.messages:
        if message["role"] == "user":
            st.chat_message("user").write(message["content"])
        elif message["role"] == "expert":
            # å°‚é–€å®¶ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
            expert_info = st.session_state.experts[message["expert_id"]]
            with st.chat_message("assistant", avatar=expert_info["avatar"]):
                st.markdown(f"**{expert_info['name']}** ã®åˆ†æ:")
                st.markdown(message["content"])
        elif message["role"] == "summary":
            # æœ€çµ‚çµ±åˆæ„è¦‹
            with st.chat_message("assistant", avatar="ğŸ¤–"):
                st.markdown("**çµ±åˆã•ã‚ŒãŸçµè«–:**")
                st.markdown(message["content"])
                st.markdown(f"*ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«: {st.session_state.model_config['model_name']} (Temperature: {st.session_state.model_config['temperature']})*")
    
    # ãƒãƒ£ãƒƒãƒˆå…¥åŠ›ã¨å‡¦ç†
    if prompt := st.chat_input("è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„"):
        if not api_key:
            st.error("OpenAI APIã‚­ãƒ¼ã‚’è¨­å®šã—ã¦ãã ã•ã„ï¼")
            return
        
        if not st.session_state.experts:
            st.error("å°‘ãªãã¨ã‚‚1äººã®å°‚é–€å®¶ã‚’è¨­å®šã—ã¦ãã ã•ã„ï¼")
            return
        
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã‚’è¡¨ç¤º
        st.session_state.messages.append({"role": "user", "content": prompt})
        st.chat_message("user").write(prompt)
        
        # ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ä½œæˆ
        chatbot = ExpertPanelChatbot(
            api_key,
            st.session_state.experts,
            st.session_state.model_config
        )
        
        # å„å°‚é–€å®¶ã®æ„è¦‹ã‚’é †ç•ªã«å–å¾—ã—ã¦è¡¨ç¤º
        expert_opinions = {}
        for expert_id, expert_info in st.session_state.experts.items():
            # å°‚é–€å®¶ãŒè€ƒãˆã¦ã„ã‚‹ã“ã¨ã‚’è¡¨ç¤º
            thinking_message = f"ğŸ’­ **{expert_info['name']}** ãŒåˆ†æä¸­..."
            with st.chat_message("assistant", avatar=expert_info["avatar"]):
                thinking_placeholder = st.empty()
                thinking_placeholder.markdown(thinking_message)
                
                # æ„è¦‹ã‚’å–å¾—
                opinion = await chatbot.get_expert_opinion(
                    expert_id,
                    expert_info["description"],
                    prompt,
                    thinking_placeholder,
                )
                expert_opinions[expert_id] = opinion
                
                # è€ƒãˆä¸­ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å®Ÿéš›ã®æ„è¦‹ã§ç½®ãæ›ãˆ
                thinking_placeholder.empty()
                st.markdown(f"**{expert_info['name']}** ã®åˆ†æ:")
                st.markdown(opinion)
            
            # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å±¥æ­´ã«è¿½åŠ 
            st.session_state.messages.append({
                "role": "expert",
                "expert_id": expert_id,
                "content": opinion
            })
        
        # çµ±åˆã•ã‚ŒãŸçµè«–ã‚’ç”Ÿæˆ
        with st.chat_message("assistant", avatar="ğŸ¤–"):
            final_response = await chatbot.get_integrated_response(prompt, expert_opinions, st)
        
              # çµ±åˆã•ã‚ŒãŸçµè«–ã‚’è¡¨ç¤º
            summary_message = {
                "role": "summary",
                "content": final_response
            }
            st.session_state.messages.append(summary_message)

            st.markdown("**çµ±åˆã•ã‚ŒãŸçµè«–:**")
            st.markdown(final_response)
            st.markdown(f"*ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«: {st.session_state.model_config['model_name']} (Temperature: {st.session_state.model_config['temperature']})*")

if __name__ == "__main__":
    asyncio.run(main())