import streamlit as st
from langchain_openai import ChatOpenAI
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain
import asyncio
import os

class ExpertPanelChatbot:
    def __init__(self, openai_api_key: str):
        self.llm = ChatOpenAI(
            model_name="gpt-4o-mini",
            temperature=0.7,
            openai_api_key=openai_api_key
        )
        
        self.experts = {
            "business_analyst": "ãƒ“ã‚¸ãƒã‚¹ã‚¢ãƒŠãƒªã‚¹ãƒˆã€‚å¸‚å ´åˆ†æã€åç›Šæ€§ã€ãƒ“ã‚¸ãƒã‚¹æˆ¦ç•¥ã®è¦³ç‚¹ã‹ã‚‰åˆ†æã‚’è¡Œã†ã€‚",
            "tech_expert": "æŠ€è¡“å°‚é–€å®¶ã€‚æŠ€è¡“çš„ãªå®Ÿç¾å¯èƒ½æ€§ã€å¿…è¦ãªãƒªã‚½ãƒ¼ã‚¹ã€é–‹ç™ºæœŸé–“ã«ã¤ã„ã¦åˆ†æã‚’è¡Œã†ã€‚",
            "risk_manager": "ãƒªã‚¹ã‚¯ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã€‚æ³•çš„ãƒªã‚¹ã‚¯ã€ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒªã‚¹ã‚¯ã€é‹ç”¨ãƒªã‚¹ã‚¯ã‚’è©•ä¾¡ã™ã‚‹ã€‚",
            "customer_advocate": "ã‚«ã‚¹ã‚¿ãƒãƒ¼ã‚¢ãƒ‰ãƒœã‚±ã‚¤ãƒˆã€‚é¡§å®¢è¦–ç‚¹ã§ã®ä¾¡å€¤ã€ä½¿ã„ã‚„ã™ã•ã€éœ€è¦ã‚’è©•ä¾¡ã™ã‚‹ã€‚"
        }
        
        self.expert_chains = self._initialize_expert_chains()
        self.summary_chain = self._initialize_summary_chain()
        
    def _initialize_expert_chains(self) -> dict:
        expert_chains = {}
        expert_prompt_template = """
        ã‚ãªãŸã¯{expert_role}ã¨ã—ã¦ã€ä»¥ä¸‹ã®è³ªå•/ãƒˆãƒ”ãƒƒã‚¯ã«ã¤ã„ã¦å°‚é–€çš„ãªæ„è¦‹ã‚’æä¾›ã—ã¦ãã ã•ã„ã€‚
        
        è³ªå•/ãƒˆãƒ”ãƒƒã‚¯: {question}
        
        å›ç­”ã®éš›ã¯ä»¥ä¸‹ã®ç‚¹ã«æ³¨æ„ã—ã¦ãã ã•ã„ï¼š
        - ã‚ãªãŸã®å°‚é–€åˆ†é‡ã®è¦³ç‚¹ã‹ã‚‰ã®åˆ†æã‚’æä¾›ã™ã‚‹ã“ã¨
        - å…·ä½“çš„ãªæ ¹æ‹ ã‚„ä¾‹ã‚’ç¤ºã™ã“ã¨
        - æ½œåœ¨çš„ãªèª²é¡Œã‚„æ©Ÿä¼šã‚’æŒ‡æ‘˜ã™ã‚‹ã“ã¨
        
        å°‚é–€å®¶ã¨ã—ã¦ã®æ„è¦‹:
        """
        
        for expert_id, expert_desc in self.experts.items():
            prompt = PromptTemplate(
                template=expert_prompt_template,
                input_variables=["expert_role", "question"]
            )
            expert_chains[expert_id] = LLMChain(llm=self.llm, prompt=prompt)
        return expert_chains
    
    def _initialize_summary_chain(self) -> LLMChain:
        summary_prompt_template = """
        ä»¥ä¸‹ã¯ç•°ãªã‚‹å°‚é–€å®¶ã‹ã‚‰æä¾›ã•ã‚ŒãŸæ„è¦‹ã§ã™ã€‚ã“ã‚Œã‚‰ã®æ„è¦‹ã‚’çµ±åˆã—ã€ãƒãƒ©ãƒ³ã‚¹ã®å–ã‚ŒãŸåŒ…æ‹¬çš„ãªçµè«–ã‚’å°ãå‡ºã—ã¦ãã ã•ã„ã€‚
        
        å…ƒã®è³ªå•/ãƒˆãƒ”ãƒƒã‚¯: {original_question}
        
        å°‚é–€å®¶ã®æ„è¦‹:
        {expert_opinions}
        
        çµ±åˆã•ã‚ŒãŸçµè«–:
        1. ä¸»ãªåˆæ„ç‚¹
        2. é‡è¦ãªæ‡¸å¿µäº‹é …
        3. æ¨å¥¨ã•ã‚Œã‚‹ã‚¢ã‚¯ã‚·ãƒ§ãƒ³
        4. è¿½åŠ ã§æ¤œè¨ãŒå¿…è¦ãªç‚¹
        
        çµè«–:
        """
        
        summary_prompt = PromptTemplate(
            template=summary_prompt_template,
            input_variables=["original_question", "expert_opinions"]
        )
        return LLMChain(llm=self.llm, prompt=summary_prompt)
    
    async def get_expert_opinions(self, question: str) -> dict:
        opinions = {}
        for expert_id, expert_desc in self.experts.items():
            response = await self.expert_chains[expert_id].arun(
                expert_role=expert_desc,
                question=question
            )
            opinions[expert_id] = response
        return opinions
    
    async def get_integrated_response(self, question: str) -> str:
        expert_opinions = await self.get_expert_opinions(question)
        formatted_opinions = "\n\n".join([
            f"{expert_id.upper()}ã®æ„è¦‹:\n{opinion}"
            for expert_id, opinion in expert_opinions.items()
        ])
        
        final_response = await self.summary_chain.arun(
            original_question=question,
            expert_opinions=formatted_opinions
        )
        return final_response, expert_opinions

def init_session_state():
    if 'messages' not in st.session_state:
        st.session_state.messages = []
    if 'chatbot' not in st.session_state:
        st.session_state.chatbot = None

async def main():
    st.title("å°‚é–€å®¶ãƒ‘ãƒãƒ«ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆ ğŸ¤–")
    
    init_session_state()
    
    # ã‚µã‚¤ãƒ‰ãƒãƒ¼ã§APIã‚­ãƒ¼ã‚’è¨­å®š
    with st.sidebar:
        st.header("è¨­å®š")
        api_key = st.text_input("OpenAI APIã‚­ãƒ¼", type="password")
        if api_key:
            st.session_state.chatbot = ExpertPanelChatbot(api_key)
            st.success("APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¾ã—ãŸï¼")
    
    # ãƒ¡ã‚¤ãƒ³ã®ãƒãƒ£ãƒƒãƒˆã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹
    for message in st.session_state.messages:
        if message["role"] == "user":
            st.chat_message("user").write(message["content"])
        else:
            with st.chat_message("assistant"):
                st.write(message["content"])
                if "expert_opinions" in message:
                    with st.expander("å„å°‚é–€å®¶ã®è©³ç´°ãªæ„è¦‹ã‚’è¦‹ã‚‹"):
                        for expert, opinion in message["expert_opinions"].items():
                            st.subheader(f"ğŸ’¡ {expert.replace('_', ' ').title()}")
                            st.write(opinion)
    
    # ãƒãƒ£ãƒƒãƒˆå…¥åŠ›
    if prompt := st.chat_input("è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„"):
        if not api_key:
            st.error("OpenAI APIã‚­ãƒ¼ã‚’è¨­å®šã—ã¦ãã ã•ã„ï¼")
            return
        
        st.session_state.messages.append({"role": "user", "content": prompt})
        st.chat_message("user").write(prompt)
        
        with st.chat_message("assistant"):
            with st.spinner("å°‚é–€å®¶ã«æ„è¦‹ã‚’èã„ã¦ã„ã¾ã™..."):
                response, expert_opinions = await st.session_state.chatbot.get_integrated_response(prompt)
                st.write(response)
                with st.expander("å„å°‚é–€å®¶ã®è©³ç´°ãªæ„è¦‹ã‚’è¦‹ã‚‹"):
                    for expert, opinion in expert_opinions.items():
                        st.subheader(f"ğŸ’¡ {expert.replace('_', ' ').title()}")
                        st.write(opinion)
                
                st.session_state.messages.append({
                    "role": "assistant",
                    "content": response,
                    "expert_opinions": expert_opinions
                })

if __name__ == "__main__":
    asyncio.run(main())